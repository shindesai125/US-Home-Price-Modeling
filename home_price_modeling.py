# -*- coding: utf-8 -*-
"""US Home analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZNye_bWXHZVD2v5_dPM_Myf7EZIXCFou

### **Starting with the 1st Dataset i.e. ('/content/1 Home Prices.csv)**
"""

import pandas as pd
import matplotlib.pyplot as plt
df1=pd.read_csv("/content/1 Home Prices.csv")
df1.head()

# Convert date column to datetime
df1['observation_date'] = pd.to_datetime(df1['observation_date'])

# Set date as index
df1.set_index('observation_date', inplace=True)

# Rename column for clarity
df1.rename(columns={'CSUSHPISA': 'Home_Price_Index'}, inplace=True)

# Filter to 2005–2025
df1 = df1['2005-01-01':'2025-01-01']

df1.head()

# Plot the data
df1.plot(title="S&P Case-Shiller U.S. Home Price Index", figsize=(10, 5))

"""# **Working on the 2nd Dataset i.e. ('/content/2 Mortgage Rates.csv')**"""

df2=pd.read_csv("/content/2 Mortgage Rates.csv")
df2.head()

# Convert to datetime
df2['observation_date'] = pd.to_datetime(df2['observation_date'])

# Set index
df2.set_index('observation_date', inplace=True)

# Rename column
df2.rename(columns={'MORTGAGE30US': 'Mortgage_Rate'}, inplace=True)

# ✅ Filter to 2005–2025
df2 = df2['2005-01-01':'2025-01-01']

df2.head()

# checking the min and max values in the date range
df2.index.min(), df2.index.max()

# Plot to visually inspect
df2.plot(title='30 Year Fixed Mortgage Rate(%)',figsize=(10, 5))

"""### **Working on the 3rd Dataset i.e.('/content/3 Unemployment Rate.csv')**

---
"""

df3=pd.read_csv("/content/3 Unemployment Rate.csv")
df3.head()

# Convert date column
df3['observation_date'] = pd.to_datetime(df3['observation_date'])

# Set index
df3.set_index('observation_date', inplace=True)

# Rename column
df3.rename(columns={'UNRATE':'Unemployment_Rate'}, inplace=True)

# Filter to match the older datasets
df3=df3['2005-01-01':'2025-01-01']

df3.head()

df3.plot(title="U.S. Unemployment Rate (%)", figsize=(10,5))

"""# **Working with the 4th Dataset i.e.('/content/4 Median Household Income.xlsx'**"""

#Load the raw dataset
df4_raw=pd.read_excel('/content/4 Median Household Income.xlsx',skiprows=8)
df4_raw.head(100)

# Select and rename only the necessary columns
df4 = df4_raw[['Unnamed: 0', '2023\ndollars']].copy()
df4.columns = ['Year', 'Median_Income_2023Dollars']

# Drop any rows with missing or non-numeric year/income
df4 = df4[pd.to_numeric(df4['Year'], errors='coerce').notnull()]

# Convert 'Median_Income_2023Dollars' to numeric, coercing errors
df4['Median_Income_2023Dollars'] = pd.to_numeric(df4['Median_Income_2023Dollars'], errors='coerce')

df4 = df4.dropna(subset=['Median_Income_2023Dollars'])

# Convert Year to datetime and set as index
df4['Year'] = pd.to_datetime(df4['Year'].astype(int), format='%Y')
df4.set_index('Year', inplace=True)

#Check for duplicates in the index
print("Duplicate years:", df4.index.duplicated().sum())

#Drop duplicate years if any
df4 = df4[~df4.index.duplicated(keep='first')]

#Resample to monthly using 'ME' (month-end) and interpolate
df4 = df4.resample('ME').interpolate()

#Preview
df4.head(12)

"""## **Working with the 5th dataset i.e.('/content/5 Housing Supply.csv')**"""

df5=pd.read_csv("/content/5 Housing Supply.csv")
df5.head()

# Convert date column
df5['observation_date'] = pd.to_datetime(df5['observation_date'])

# Set index
df5.set_index('observation_date', inplace=True)

# Rename column
df5.rename(columns={'HOUST': 'Housing_Starts'}, inplace=True)

# Sorrt by date
df5.sort_index(inplace=True)

# Preview
df5.head()

"""# **Working with the 6th Dataset i.e.('/content/6 CPI.csv')**


"""

df6=pd.read_csv('/content/6 CPI.csv')
df6.head()

#Convert 'observation_date' to datetime format
df6['observation_date'] = pd.to_datetime(df6['observation_date'])

#Rename columns for clarity
df6 = df6.rename(columns={'observation_date': 'Date', 'CPIAUCSL': 'CPI'})

#Filter data for the last 20 years only (from 2004 onwards)
df6 = df6[df6['Date'] >= '2004-01-01']

# Reseting index
df6 = df6.reset_index(drop=True)

#Preview
df6.head()

"""# **Wrking with the 7th Dataset i.e.('/content/7 NA-EST2024-POP(population estimates 2020-2025).xlsx')**"""

df7=pd.read_excel('/content/7 NA-EST2024-POP_cleaned.xlsx')
df7.head(5)

#Keep only relevant columns
df7 = df7[['Year and Month', 'Resident Population']]

# Rename columns
df7.columns = ['Date', 'Population']

# Convert Date to datetime
df7['Date'] = pd.to_datetime(df7['Date'])

# Remove commas from Population and convert to integer
df7['Population'] = df7['Population'].astype(str).str.replace(',', '').astype(int)

# Set Date as index (optional but helpful)
df7.set_index('Date', inplace=True)

# Preview cleaned DataFrame
df7.head()

"""# **Working with the 8th dataset i.e.('/content/8 NST-EST2020(population totals 2000-2020).xlsx'**"""

import pandas as pd

# Load and skip the first row (header explanation)
df = pd.read_excel("/content/8 NST-EST2020(population totals 2000-2020).xlsx", skiprows=1)

# These are July 1 population estimate columns from 2010 to 2020
july_columns = [
    'Population Estimate (as of July 1)', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6',
    'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10',
    'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 14'  # July 1, 2020
]

# Replace with actual years
year_labels = [str(y) for y in range(2010, 2021)]

# Subset and rename columns
df_july = df[['Geographic Area'] + july_columns].copy()
df_july.columns = ['Geographic Area'] + year_labels

# Convert from wide to long format (year as row)
df_long = df_july.melt(id_vars='Geographic Area', var_name='Year', value_name='Population')

# Clean the population values and convert date
# Step 4: Remove NaN and non-numeric population values
df_long = df_long.dropna(subset=['Population'])  # Drop rows where Population is NaN

# Remove commas and keep only numeric strings
df_long = df_long[df_long['Population'].astype(str).str.replace(',', '').str.isnumeric()]

# Convert cleaned population column to int
df_long['Population'] = df_long['Population'].astype(str).str.replace(',', '').astype(int)

# Add date column
df_long['Date'] = pd.to_datetime(df_long['Year'] + '-07-01')

# Set index or reorder
df_long = df_long[['Date', 'Geographic Area', 'Population']].sort_values(['Geographic Area', 'Date'])
# df_long.set_index('Date', inplace=True)  # optional

df8 = df_long[df_long['Geographic Area'] == 'United States'].copy()
df8.reset_index(drop=True, inplace=True)

df8

df7_raw = pd.read_excel("/content/7 NA-EST2024-POP_cleaned.xlsx", header=None)
df7_raw.head(15)

# Load with correct header
df7 = pd.read_excel("/content/7 NA-EST2024-POP_cleaned.xlsx", header=0)

# Select and rename necessary columns
df7_cleaned = df7[['Year and Month', 'Resident Population']].copy()
df7_cleaned.columns = ['Date', 'Population']

# Convert 'Date' to datetime format
df7_cleaned['Date'] = pd.to_datetime(df7_cleaned['Date'])

# Add source label for tracking
df7_cleaned['Source'] = 'Monthly Census Estimates'

# Preview
df7_cleaned.head()

#Ensure both datasets have the same column names
df8_cleaned = df_long[['Date', 'Geographic Area', 'Population']].copy()
df8_cleaned = df8_cleaned[df8_cleaned['Geographic Area'] == 'United States']
df8_cleaned['Source'] = 'July 1 Annual Census Estimates'
df8_cleaned = df8_cleaned.drop(columns='Geographic Area')

# Align df7_cleaned column order
df7_cleaned = df7_cleaned[['Date', 'Population', 'Source']]

# Combine both
population_combined = pd.concat([df8_cleaned, df7_cleaned]).sort_values('Date').reset_index(drop=True)

# Final preview
population_combined.head(10)

"""# **merging all cleaned DataFrames into a single master dataset based on the Date column**"""

df1 = pd.read_csv('/content/1 Home Prices.csv')
print(df1.head())
print(df1.columns)
df1.columns = ['Date', 'Home_Price_Index']
df1['Date'] = pd.to_datetime(df1['Date'])
# Rename columns properly in df1
df1.rename(columns={'observation_date': 'Date', 'CSUSHPISA': 'Home_Price_Index'}, inplace=True)

# Create master_df with the renamed columns
master_df = df1.copy()
print(master_df.head())

# If Date is an index, reset it
df2 = df2.reset_index()
master_df = master_df.reset_index()

# Then rename
df2.rename(columns={'observation_date': 'Date', 'MORTGAGE30US': 'Mortgage_Rate'}, inplace=True)

# Confirm both contain 'Date'
print('df2 columns:', df2.columns)
print('master_df columns:', master_df.columns)

# Merge
master_df = pd.merge(master_df, df2, on='Date', how='left')

# Rename if not already
df2.rename(columns={'observation_date': 'Date', 'MORTGAGE30US': 'Mortgage_Rate'}, inplace=True)

# Merge into master_df
master_df = pd.merge(master_df, df2, on='Date', how='left')

# print("master_df:", master_df.columns.tolist())
print("df2:", df2.columns.tolist())

print(df2.columns.tolist())       # Confirm visible columns
print(df2.index.name)             # Check if 'observation_date' is in the index
print(df2.head())                 # View top 5 rows

# Reset index so observation_date becomes a column
df2 = df2.reset_index()

# Then rename properly
df2.rename(columns={'observation_date': 'Date', 'MORTGAGE30US': 'Mortgage_Rate'}, inplace=True)

# Merge into master_df
master_df = pd.merge(master_df, df2, on='Date', how='left')

print(df3.columns)

df3.rename(columns={'observation_date': 'Date', 'UNRATE': 'Unemployment_Rate'}, inplace=True)

# --- df3: Unemployment Rate ---
if 'observation_date' in df3.columns:
    df3 = df3.rename(columns={'observation_date': 'Date'})
elif 'DATE' in df3.columns:
    df3 = df3.rename(columns={'DATE': 'Date'})
df3['Date'] = pd.to_datetime(df3['Date'], errors='coerce')
df3 = df3.dropna(subset=['Date'])
df3 = df3[['Date', 'Unemployment_Rate']]
master_df = master_df.drop(columns=['Unemployment_Rate'], errors='ignore')
master_df = pd.merge(master_df, df3, on='Date', how='left')

# --- df4: Median Household Income ---
df4 = df4.drop(columns=['level_0', 'index'], errors='ignore')
df4 = df4.reset_index(drop=True)
if 'Year' in df4.columns and 'Median_Income_2023Dollars' in df4.columns:
    # Create a monthly date for merging
    df4['Date'] = pd.to_datetime(df4['Year'].astype(str) + '-01-01')
    df4 = df4[['Date', 'Median_Income_2023Dollars']]
master_df = master_df.drop(columns=['Median_Income_2023Dollars'], errors='ignore')
master_df = pd.merge(master_df, df4, on='Date', how='left')

# --- df5: Housing Supply / Starts ---
df5 = df5.drop(columns=['level_0', 'index'], errors='ignore')
df5 = df5.reset_index(drop=True)

# Identify and convert appropriate date field
if 'Year' in df5.columns and 'Housing_Starts' in df5.columns:
    df5['Date'] = pd.to_datetime(df5['Year'].astype(str) + '-01-01')
elif 'DATE' in df5.columns:
    df5.rename(columns={'DATE': 'Date'}, inplace=True)
    df5['Date'] = pd.to_datetime(df5['Date'])
elif 'Date' not in df5.columns:
    print("⚠️ ERROR: df5 has no recognized date column like 'Year', 'DATE', or 'Date'")
    print("df5.columns:", df5.columns)

# Now select only the required columns
if 'Date' in df5.columns and 'Housing_Starts' in df5.columns:
    df5 = df5[['Date', 'Housing_Starts']]
    master_df = master_df.drop(columns=['Housing_Starts'], errors='ignore')
    master_df = pd.merge(master_df, df5, on='Date', how='left')
else:
    print("⚠️ Skipping df5 merge: Required columns missing.")


# --- df6: CPI ---
df6 = df6.drop(columns=['level_0', 'index'], errors='ignore')
df6 = df6.reset_index(drop=True)
if 'DATE' in df6.columns:
    df6.rename(columns={'DATE': 'Date'}, inplace=True)
df6['Date'] = pd.to_datetime(df6['Date'])
df6 = df6[['Date', 'CPI']]
master_df = master_df.drop(columns=['CPI'], errors='ignore')
master_df = pd.merge(master_df, df6, on='Date', how='left')

# --- df7_cleaned: Monthly Population ---
df7_cleaned.columns = ['Date', 'Population_Monthly', 'Source']
df7_cleaned['Date'] = pd.to_datetime(df7_cleaned['Date'])
master_df = master_df.drop(columns=['Population_Monthly', 'Source'], errors='ignore')
master_df = pd.merge(master_df, df7_cleaned, on='Date', how='left')

# --- df_long: Annual Population ---
df_long = df_long[df_long['Geographic Area'] == 'United States']
df_long = df_long[['Date', 'Population']]
df_long.columns = ['Date', 'Population_Annual']
df_long['Date'] = pd.to_datetime(df_long['Date'])
master_df = master_df.drop(columns=['Population_Annual'], errors='ignore')
master_df = pd.merge(master_df, df_long, on='Date', how='left')

print("master_df columns:", master_df.columns.tolist())
print("df3 columns:", df3.columns.tolist())

master_df = master_df.sort_values('Date')
master_df.reset_index(drop=True, inplace=True)

master_df.head()

master_df.to_csv('/content/master_df1.csv', index=False)

import pandas as pd
master_df=pd.read_csv('/content/master_df1.csv')
master_df.head()

print(master_df.shape)
print(master_df.dtypes)
print(master_df.columns.tolist())
master_df.head(3)

# Basic info
master_df.info()

# Preview numeric stats
master_df.describe()

# Drop redundant columns
master_df = master_df.drop(columns=['CPI_x', 'CPI_y', 'index_x', 'index_y'], errors='ignore')

master_df['Date'] = pd.to_datetime(master_df['Date'])
master_df = master_df.sort_values('Date')

master_df.isnull().sum()

# Forward-fill Mortgage Rate and Unemployment
master_df['Mortgage_Rate'] = master_df['Mortgage_Rate'].ffill()
master_df['Unemployment_Rate'] = master_df['Unemployment_Rate'].ffill()

# Forward-fill or interpolate Population_Monthly
master_df['Population_Monthly'] = master_df['Population_Monthly'].interpolate(method='linear')

import matplotlib.pyplot as plt

# Plot all main indicators
master_df.set_index('Date')[['Home_Price_Index', 'Mortgage_Rate', 'Unemployment_Rate',
                             'Housing_Starts', 'CPI', 'Population_Monthly', 'Population_Annual']].plot(subplots=True, figsize=(14,12))
plt.tight_layout()
plt.show()

import seaborn as sns

plt.figure(figsize=(10, 8))
sns.heatmap(master_df.corr(numeric_only=True), annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

master_df['Mortgage_Rate'] = master_df['Mortgage_Rate'].ffill().bfill()

master_df['Population_Monthly'] = master_df['Population_Monthly'].interpolate(method='linear')

master_df = master_df.drop(columns=['Source', 'Population_Annual'], errors='ignore')

master_df.isnull().sum()

master_df['Date'] = pd.to_datetime(master_df['Date'])
master_df = master_df.sort_values('Date').reset_index(drop=True)

master_df['Population_Monthly'] = master_df['Population_Monthly'].interpolate(method='linear')

master_df['Population_Monthly'] = master_df['Population_Monthly'].interpolate(method='linear', limit_direction='both')

master_df.isnull().sum()

master_df.info()
master_df.describe()
master_df.head()

master_df.duplicated().sum()

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
sns.heatmap(master_df.corr(numeric_only=True), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

plt.figure(figsize=(14, 6))
plt.plot(master_df['Date'], master_df['Home_Price_Index'], label='Home Price Index')
plt.plot(master_df['Date'], master_df['Mortgage_Rate'], label='Mortgage Rate')
plt.plot(master_df['Date'], master_df['Unemployment_Rate'], label='Unemployment Rate')
plt.legend()
plt.title('Trends Over Time')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

sns.histplot(master_df['Home_Price_Index'], kde=True)
plt.title('Distribution of Home Price Index')
plt.show()

sns.histplot(master_df['Mortgage_Rate'], kde=True)
plt.title('Distribution of Mortgage_Rate')
plt.show()

sns.histplot(master_df['Unemployment_Rate'], kde=True)
plt.title('Distribution of Unemployment_Rate')
plt.show()

sns.histplot(master_df['CPI'], kde=True)
plt.title('Distribution of CPI')
plt.show()

sns.boxplot(x=master_df['Mortgage_Rate'])
plt.title('Mortgage Rate Outliers')
plt.show()

sns.boxplot(x=master_df['Housing_Starts'])
plt.title('Housing_Starts Outliers')
plt.show()

sns.boxplot(x=master_df['CPI'])
plt.title('CPI Outliers')
plt.show()

sns.boxplot(x=master_df['Unemployment_Rate'])
plt.title('Unemployment_Rate Outliers')
plt.show()

master_df['HPI_rolling'] = master_df['Home_Price_Index'].rolling(window=12).mean()

plt.plot(master_df['Date'], master_df['Home_Price_Index'], label='Original')
plt.plot(master_df['Date'], master_df['HPI_rolling'], label='12-Month Rolling Mean', color='orange')
plt.legend()
plt.title('Home Price Index - Rolling Average')
plt.show()

"""# **Feature Engineering**"""

import numpy as np
master_df['Year'] = master_df['Date'].dt.year
master_df['Month'] = master_df['Date'].dt.month

master_df['Post_COVID'] = master_df['Date'].apply(lambda x: 1 if x >= pd.to_datetime('2020-03-01') else 0)

master_df['Log_HPI'] = np.log(master_df['Home_Price_Index'])
master_df['Log_Unemployment'] = np.log1p(master_df['Unemployment_Rate'])

master_df['CPI_Growth'] = master_df['CPI'].pct_change().fillna(0)

master_df.columns

master_df['Mortgage_Rate_Level'] = pd.cut(
    master_df['Mortgage_Rate'],
    bins=[0, 3.5, 5, 7],
    labels=['Low', 'Medium', 'High']
)

features = [
    'Mortgage_Rate',
    'Log_Unemployment',
    'Housing_Starts',
    'CPI',
    'CPI_Growth',
    'Population_Monthly',
    'Post_COVID',
    'Year',
    'Month'
]

target = 'Log_HPI'

master_df[features + [target]].info()

"""# **MODELING PHASE**"""

from sklearn.model_selection import train_test_split

X = master_df[[
    'Mortgage_Rate',
    'Log_Unemployment',
    'Housing_Starts',
    'CPI',
    'CPI_Growth',
    'Population_Monthly',
    'Post_COVID',
    'Year',
    'Month'
]]
y = master_df['Log_HPI']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

y_pred = lr_model.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"Linear Regression RMSE: {rmse:.4f}")
print(f"Linear Regression R²: {r2:.4f}")

from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

rf_pred = rf_model.predict(X_test)

rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))
rf_r2 = r2_score(y_test, rf_pred)

print(f"Random Forest RMSE: {rf_rmse:.4f}")
print(f"Random Forest R²: {rf_r2:.4f}")

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

feature_importance = pd.Series(rf_model.feature_importances_, index=X.columns)
feature_importance.sort_values(ascending=True).plot(kind='barh', figsize=(8,5))
plt.title("Random Forest Feature Importance")
plt.tight_layout()
plt.show()

"""# **Train-Test Split**"""

from sklearn.model_selection import train_test_split

# Define your features (X) and target (y)
X = master_df.drop(columns=['Log_HPI'])   # Drop the target column
y = master_df['Log_HPI']                  # Use log-transformed HPI as the target

# Split into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Missing values in X_train:")
print(X_train.isnull().sum())

X_train = X_train.fillna(X_train.median(numeric_only=True))

if 'Date' in X_train.columns:
    X_train = X_train.drop(columns='Date')

print(X_train.dtypes)

# Reset column to original form
def label_mortgage_level(rate):
    if rate < 4:
        return 'Low'
    elif rate < 6:
        return 'Medium'
    else:
        return 'High'

X_train['Mortgage_Rate_Level'] = X_train['Mortgage_Rate'].apply(label_mortgage_level)
X_test['Mortgage_Rate_Level'] = X_test['Mortgage_Rate'].apply(label_mortgage_level)

from sklearn.preprocessing import OrdinalEncoder

# Convert to string just to be safe
X_train['Mortgage_Rate_Level'] = X_train['Mortgage_Rate_Level'].astype(str)
X_test['Mortgage_Rate_Level'] = X_test['Mortgage_Rate_Level'].astype(str)

# Encode the column
encoder = OrdinalEncoder(categories=[['Low', 'Medium', 'High']])
X_train['Mortgage_Rate_Level'] = encoder.fit_transform(X_train[['Mortgage_Rate_Level']])
X_test['Mortgage_Rate_Level'] = encoder.transform(X_test[['Mortgage_Rate_Level']])

lr = LinearRegression()
lr.fit(X_train, y_train)

rf = RandomForestRegressor(random_state=42)
rf.fit(X_train, y_train)

# Drop 'Date' if present
X_test = X_test.drop(columns=['Date'], errors='ignore')

# Then predict
y_pred_rf = rf.predict(X_test)

# Now compute R²
from sklearn.metrics import r2_score

r2 = r2_score(y_test, y_pred_rf)
print(f"Random Forest R² Score: {r2:.4f}")

from sklearn.metrics import mean_squared_error
import numpy as np

rmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))
print(f"Random Forest RMSE: {rmse:.2f}")

# Linear Regression predictions
y_pred_lr = lr.predict(X_test)

# Metrics
r2_lr = r2_score(y_test, y_pred_lr)
rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))

print(f"Linear Regression R² Score: {r2_lr:.4f}")
print(f"Linear Regression RMSE: {rmse_lr:.2f}")

"""# **Residual Analysis for Both Models**"""

y_pred_lr = lr.predict(X_test)
y_pred_rf = rf.predict(X_test)

residuals_lr = y_test - y_pred_lr
residuals_rf = y_test - y_pred_rf

import matplotlib.pyplot as plt
import seaborn as sns

# Linear Regression Residuals
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.scatterplot(x=y_pred_lr, y=residuals_lr)
plt.axhline(0, color='red', linestyle='--')
plt.title("Linear Regression Residuals")
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")

plt.subplot(1, 2, 2)
sns.histplot(residuals_lr, kde=True, bins=30)
plt.title("Residual Distribution - Linear Regression")
plt.xlabel("Residuals")

plt.tight_layout()
plt.show()

# Random Forest Residuals
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.scatterplot(x=y_pred_rf, y=residuals_rf)
plt.axhline(0, color='red', linestyle='--')
plt.title("Random Forest Residuals")
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")

plt.subplot(1, 2, 2)
sns.histplot(residuals_rf, kde=True, bins=30)
plt.title("Residual Distribution - Random Forest")
plt.xlabel("Residuals")

plt.tight_layout()
plt.show()

"""# **Feature Importance (Random Forest)-Understanding which variables mattered most.**"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Get feature importances from the model
importances = rf.feature_importances_

# Create a DataFrame for better visualization
feature_importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# Display the table
print(feature_importance_df)

# Plot
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis')
plt.title('Feature Importances from Random Forest')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

"""# **Train Model with Top Features Only**"""

top_features = ['Home_Price_Index', 'HPI_rolling', 'CPI', 'Population_Monthly']
X_train_top = X_train[top_features]
X_test_top = X_test[top_features]

# Retrain model on top features
rf_top = RandomForestRegressor(random_state=42)
rf_top.fit(X_train_top, y_train)

# Evaluate
from sklearn.metrics import mean_squared_error
import numpy as np

mse = mean_squared_error(y_test, y_pred_top)
rmse = np.sqrt(mse)

print(f"R² Score (Top Features): {r2_score(y_test, y_pred_top):.4f}")
print(f"RMSE (Top Features): {rmse:.4f}")

# Save Final Model
import joblib
joblib.dump(rf, "final_random_forest_model.pkl")

from google.colab import files
files.download('final_random_forest_model.pkl')